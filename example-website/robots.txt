# robots.txt for example-website

# Metadata for webcrawler
Payment-url: http://localhost:3000/v1/crawler/uuid

# Directives for AI crawlers
User-agent: *
Paid-content: /blog/
Paid-content: /products/
